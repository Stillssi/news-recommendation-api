{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "host_name = 'localhost'\n",
    "host_port = 3306\n",
    "username='root'\n",
    "password='12345678'\n",
    "database_name='news_team_4'\n",
    "db = pymysql.connect(\n",
    "    host=host_name,\n",
    "    port=host_port,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    db=database_name,\n",
    "    charset='utf8'\n",
    ")\n",
    "SQL='SELECT * FROM tb_news'\n",
    "df=pd.read_sql(SQL, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "import re\n",
    "def preprocessing(data):\n",
    "    text=re.sub('[①②③④⑤⑥⑦⑧⑨⑩○●〈〉@◀‘’“”,\"?]','',data).replace('\\xa0','')\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "def article_preprocessing(news_df):\n",
    "    '''\n",
    "    데이터 프레임 각 기사 내용 전처리\n",
    "    '''\n",
    "    news_df['pre_text'] = ''\n",
    "    c= news_df['Content']\n",
    "    content = pd.DataFrame(data=c, columns=['Content'])\n",
    "    content['Content'].apply(lambda x: str(x))\n",
    "    news_df['pre_text'] = content['Content'].apply(lambda x : preprocessing(x))\n",
    "    print(news_df)\n",
    "    return news_df\n",
    "result_df = article_preprocessing(df) #원래 데이터 프레임 + 전처리된 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(result_df)):\n",
    "    a = result_df['pre_text'].iloc[i]\n",
    "    a = a.split('.')\n",
    "    try:\n",
    "        a.remove('')\n",
    "    except:\n",
    "        pass\n",
    "    result_df['pre_text'].iloc[i] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textrank import KeysentenceSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def subword_tokenizer(sent, n=3):\n",
    "    def subword(token, n):\n",
    "        if len(token) <= n:\n",
    "            return [token]\n",
    "        return [token[i:i+n] for i in range(len(token) - n + 1)]\n",
    "    return [sub for token in sent.split() for sub in subword(token, n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = KeysentenceSummarizer(\n",
    "    tokenize = subword_tokenizer,\n",
    "    min_sim = 0.3,\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['result'] = ''\n",
    "for i in range(len(result_df)):\n",
    "    text_list = []\n",
    "    \n",
    "    try:\n",
    "        print(i)\n",
    "        bias = np.ones(len(result_df['pre_text'].iloc[i]))\n",
    "        bias[-1] = 10\n",
    "        a = result_df['pre_text'].iloc[i]\n",
    "        keysents = summarizer.summarize(a, topk=3, bias=bias)\n",
    "        for _, _, sent in keysents:\n",
    "            text_list.append(sent)\n",
    "        re = ' '.join(text_list)\n",
    "        result_df['result'].iloc[i] = re\n",
    "    except:\n",
    "        result_df['result'].iloc[i] = ''.join(result_df['pre_text'].iloc[i])\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['result'].to_csv('./threesentence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mecab_morphs(data):\n",
    "    pos_list = {'NNG', 'VA', 'VAX', 'MAG', 'NNP', 'VV+EC','VV+ETM'}\n",
    "    mecab = Mecab()\n",
    "    data_token = mecab.morphs(data)\n",
    "    f = open('/Users/stillssi/Desktop/textrank/stopwords-ko.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    stopwords_list= []\n",
    "    for l in lines:\n",
    "        l = l.replace('\\n','')\n",
    "        stopwords_list.append(l)\n",
    "    result = []\n",
    "    for token in data_token:\n",
    "        if token not in stopwords_list:\n",
    "            result.append(token)\n",
    "    token_result = ''.join(result)\n",
    "    pos_token = mecab.pos(token_result)\n",
    "    pos_result = []\n",
    "    for p in pos_token:\n",
    "        if p[1] in pos_list:\n",
    "            pos_result.append(p[0])\n",
    "    pos_result = list(set(pos_result))\n",
    "    return pos_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['complete'] = ''\n",
    "result_df['complete'] = result_df['result'].apply(lambda x : mecab_morphs(x))\n",
    "result_df['complete'] = result_df['complete'].apply(lambda x : ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df = result_df.drop(['Content', 'result', 'pre_text'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df['WritedAt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df.to_csv('./최종.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df.loc[com_df['WritedAt'] == '0000-00-00 00:00:00', 'WritedAt'] = '2022-11-15 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df = com_df.rename(columns={'complete':'Content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df.to_csv('./최종.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# MySQL Connector using pymysql\n",
    "pymysql.install_as_MySQLdb()\n",
    "import MySQLdb\n",
    "\n",
    "engine = create_engine(\"mysql+mysqldb://root:12345678@localhost:3306/news_team_4\", encoding='utf-8')\n",
    "conn = engine.connect()\n",
    "com_df.to_sql(name='tb_news_team_4', con=engine, if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('project1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb0c2b2e33a8bbc9324abe172f16f5be0a36d26d93f9b38dc3fecfa8c79d8138"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
