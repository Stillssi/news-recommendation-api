{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm  # 반복문 진행상황 보기\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "host_name = 'localhost'\n",
    "host_port = 3306\n",
    "username='root'\n",
    "password='12345678'\n",
    "database_name='news_team_4'\n",
    "db = pymysql.connect(\n",
    "    host=host_name,\n",
    "    port=host_port,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    db=database_name,\n",
    "    charset='utf8'\n",
    ")\n",
    "SQL='SELECT Content, Title FROM tb_news_team_4'\n",
    "df=pd.read_sql(SQL, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_corpus_list = []\n",
    "for i in range(len(df)):\n",
    "    content = df['Content'].iloc[i]\n",
    "    tag = df['Title'].iloc[i]\n",
    "    tagged_corpus_list.append(TaggedDocument(tags=[tag], words=content))\n",
    "    if i ==70000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MonitorCallback(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.last = time.time()\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print('epoch ends {}: {}'.format(self.epoch, time.time() - self.last))\n",
    "        self.epoch += 1\n",
    "        self.last = time.time()\n",
    "\n",
    "\n",
    "# to see all data\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 450)\n",
    "pd.set_option('display.colheader_justify', 'left')\n",
    "\n",
    "# tagged_corpus_list = df['Content'].to_list()\n",
    "# print(tagged_corpus_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=100, alpha=0.025, min_alpha=0.01, workers=6, window=2, callbacks=[MonitorCallback()])\n",
    "model.build_vocab(tagged_corpus_list)\n",
    "model.train(tagged_corpus_list, total_examples=model.corpus_count, epochs=50)\n",
    "\n",
    "\n",
    "model.save('trainedMDL.')\n",
    "loaded_model = Doc2Vec.load('trainedMDL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/53738 [00:47<89:08:56,  5.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m idx_second \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, mdoel_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     14\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m         \u001b[39m# similarity_scores.append(cos_simimlarity(content_first, loaded_model.dv[idx_second]))\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m         cosine_result \u001b[39m=\u001b[39m cosine_similarity([loaded_model\u001b[39m.\u001b[39;49mdv[idx]\u001b[39m.\u001b[39;49mT], [loaded_model\u001b[39m.\u001b[39;49mdv[idx_second]])\n\u001b[1;32m     17\u001b[0m         similarity_scores\u001b[39m.\u001b[39mappend(\u001b[39m*\u001b[39mcosine_result)\n\u001b[1;32m     18\u001b[0m         \u001b[39m# print(\"cosine_result : \" , idx, idx_second, *cosine_result)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/project1/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:1383\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1381\u001b[0m     Y_normalized \u001b[39m=\u001b[39m X_normalized\n\u001b[1;32m   1382\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1383\u001b[0m     Y_normalized \u001b[39m=\u001b[39m normalize(Y, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1385\u001b[0m K \u001b[39m=\u001b[39m safe_sparse_dot(X_normalized, Y_normalized\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39mdense_output)\n\u001b[1;32m   1387\u001b[0m \u001b[39mreturn\u001b[39;00m K\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/project1/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:1786\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1784\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not a supported axis\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m axis)\n\u001b[0;32m-> 1786\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1787\u001b[0m     X,\n\u001b[1;32m   1788\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49msparse_format,\n\u001b[1;32m   1789\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1790\u001b[0m     estimator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthe normalize function\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1791\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m   1792\u001b[0m )\n\u001b[1;32m   1793\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1794\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/project1/lib/python3.8/site-packages/sklearn/utils/validation.py:740\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    737\u001b[0m array_orig \u001b[39m=\u001b[39m array\n\u001b[1;32m    739\u001b[0m \u001b[39m# store whether originally we wanted numeric dtype\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m dtype_numeric \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(dtype, \u001b[39mstr\u001b[39;49m) \u001b[39mand\u001b[39;00m dtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    742\u001b[0m dtype_orig \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(array, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    743\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(dtype_orig, \u001b[39m\"\u001b[39m\u001b[39mkind\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    744\u001b[0m     \u001b[39m# not a data type (e.g. a column named dtype in a pandas DataFrame)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def cos_simimlarity(first_docu, second_docu):\n",
    "    return dot(first_docu, second_docu) / (\n",
    "            norm(first_docu) * norm(second_docu))\n",
    "\n",
    "\n",
    "\n",
    "mdoel_size = len(loaded_model.dv)\n",
    "\n",
    "\n",
    "similarity_scores = []\n",
    "similarity_scores_list = []\n",
    "for idx, content_first in enumerate(tqdm(loaded_model.dv)):\n",
    "    for idx_second in range(0, mdoel_size - 1):\n",
    "        try:\n",
    "            # similarity_scores.append(cos_simimlarity(content_first, loaded_model.dv[idx_second]))\n",
    "            cosine_result = cosine_similarity([loaded_model.dv[idx].T], [loaded_model.dv[idx_second]])\n",
    "            similarity_scores.append(*cosine_result)\n",
    "            # print(\"cosine_result : \" , idx, idx_second, *cosine_result)\n",
    "        except Exception as e:\n",
    "            print(\"Exception : \", idx, \":\", idx_second, \":\", e)\n",
    "    similarity_scores_list.append(similarity_scores)\n",
    "    # loaded_model.dv['cos_similarity']\n",
    "\n",
    "    # loaded_model.dv.vectors['similarity_scores'].iloc[idx] = similarity_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import dot\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm  # 반복문 진행상황 보기\n",
    "loaded_model = Doc2Vec.load('trainedMDL')\n",
    "\n",
    "def cos_simimlarity(first_docu, second_docu):\n",
    "    return dot(first_docu, second_docu) / (\n",
    "            norm(first_docu) * norm(second_docu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdoel_size = len(loaded_model.dv)\n",
    "similarity_scores = []\n",
    "similarity_scores_list = []\n",
    "for idx, content_first in enumerate(tqdm(loaded_model.dv)):\n",
    "    for idx_second in range(0, mdoel_size - 1):\n",
    "        try:\n",
    "            # similarity_scores.append(cos_simimlarity(content_first, loaded_model.dv[idx_second]))\n",
    "            cosine_result = cos_simimlarity([loaded_model.dv[idx].T], [loaded_model.dv[idx_second]])\n",
    "            similarity_scores.append(*cosine_result)\n",
    "            # print(\"cosine_result : \" , idx, idx_second, *cosine_result)\n",
    "        except Exception as e:\n",
    "            print(\"Exception : \", idx, \":\", idx_second, \":\", e)\n",
    "    similarity_scores_list.append(similarity_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('project1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb0c2b2e33a8bbc9324abe172f16f5be0a36d26d93f9b38dc3fecfa8c79d8138"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
